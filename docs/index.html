<html>
	<head>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>
		<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap" rel="stylesheet">
		<style>
			h1 {
				text-align: center;
			}

      p {
        line-height: 1.6;
      }

      p.small {
        line-height: 1.4;
      }

			.container {
				margin: 0 auto;
				padding: 60px 20%;
			}

			figure {
				text-align: center;
			}

			img {
				display: inline-block;
			}

			body {
        font-family: 'Inter', sans-serif; 
        font-size: 17px;
			}
		</style>
	</head>
	<body>
		<div class="container">
		<h1>N3CTAR: Neural 3D Cellular Tessellated Automata Rendering</h1>
		<div style="text-align: center;">Team 44: Annabel Ng, George Rickus, Henry Ko, Samarth Jajoo</div>

    <div style="text-align: center; margin-top: 20px; display: flex; justify-content: center; gap: 10px;">
      <a href="https://g4ce99.github.io/N3CTAR/" target="_blank" style="text-decoration: none;">
      <button style="background-color: #333; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; display: flex; align-items: center; gap: 8px;">
      tinyurl.com/n3ctar
      </button>
      </a>
      <a href="https://github.com/G4ce99/N3CTAR" target="_blank" style="text-decoration: none;">
      <button style="background-color: #333; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; display: flex; align-items: center; gap: 8px;">
        <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png" alt="GitHub Logo" width="20" height="20">
        GitHub Repo
      </button>
      </a>
      <a href="https://tinyurl.com/n3ctarVid" target="_blank" style="text-decoration: none;">
      <button style="background-color: #333; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; display: flex; align-items: center; gap: 8px;">
        <img src="https://upload.wikimedia.org/wikipedia/commons/7/75/YouTube_social_white_squircle_%282017%29.svg" alt="Video Logo" width="20" height="20">
        Watch Video
      </button>
      </a>
      <a href="https://tinyurl.com/n3ctarSlides" target="_blank" style="text-decoration: none;">
        <button style="background-color: #333; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; font-size: 16px; display: flex; align-items: center; gap: 8px;">
          View Slides
        </button>
      </a>
    </div>

    <br>

    <div style="text-align: center; margin-top: 20px;">
      <video width="900" controls>
      <source src="images/title-demo.mov" type="video/mp4">
      </video>
    </div>

    <h2 style="text-align: center;">Abstract</h2>

    <p>

    Neural Cellular Automata (NCA) is a powerful framework for simulating the evolution of cellular structures over time where each cell's state is directly influenced by its neighbors, and it has been used in various applications such as image generation, texture synthesis, and even physics and biology simulations. However, most existing work in this area has focused on 2D cellular automata or static 3D voxel grids with limited user interaction. In this project, we aim to extend the NCA framework to 3D voxel grids and create a real-time rendering pipeline that allows for dynamic user destruction of the voxel grid. We first convert an input colored triangle mesh into a 3D voxel representation and train a 3D convolutional neural network that learns to create and regenerate this voxel representation from a minimal or damaged voxel grid. The model architecture includes <b>[TODO]</b>. The final trained model is visualized with a custom interactive renderer built with <code>Vispy</code> that allows for real-time rendering of the model output and supports user destruction of the voxel grid with the mouse cursor in order to simulate damage and regeneration. 
    </p>
    
    <h2 style="text-align: center;">Technical Approach</h2>

    <div style="text-align: center;">
      <img src="images/pipeline-logo.gif" alt="Pipeline GIF" width="800">
    </div>
    <br>

      <h3>1. 3D mesh to 3D Voxel Pipeline</h3>
      <p class="small">
        We used the <a href="https://downloads.greyc.fr/Greyc3DColoredMeshDatabase/">GREYC 3D colored mesh dataset</a> which contains 15 different .PLY files. Each vertex of a mesh is represented by 3 coordinates(x,y,z) and its RGB(r,g,b). Here's an example of a few objects included in the dataset, but we chose to work with the Mario, Mario Kart, and Duck meshes. 
      </p>
      <br><br>
        <img src="images/3d_database_img.png" alt="3d database img" style="display: block; margin: auto;" width="400">
      <br>
      <p class="small">
        Since our neural network trains on voxel grids and not triangle meshes, we wrote a script to convert the colored 3D mesh into voxels stored in an .NPY file. The voxelization process starts by normalizing the triangle mesh into voxel grid space in order to fit within the given <code>resolution x resolution x resolution</code> voxel grid. We then create a blank 3D grid of voxels and then iterate through each triangle in the given mesh. For each triangle, we calculate the voxel bounding box that contains the triangle, then loop through each voxel in the bounding box and use barycentric coordinates to check if the voxel center lies within the triangle's vertices. If it does, we assign the color of the voxel to be the color of that given triangle. To compare multiple triangles that map to the same voxel, we simply select the color of the triangle with the largest area that the voxel center lies in to be the color of the voxel. Here's an example below of our voxelization:
      </p>
      <div style="text-align:center;">
        <figure style="display:inline-block; margin:10px;"><img src="images/mario_mesh.png" width="150"><figcaption>3D Mesh Mario</figcaption></figure>
        <figure style="display:inline-block; margin:10px;"><img src="images/mario_env64.png" width="180"><figcaption>3D Voxelized Mario</figcaption></figure>
      </div>
      <p class="small">
      We also implemented a simple <code>FloodFill</code> algorithm to fill in the empty voxels inside the voxel object. The FloodFill algorithm starts at an exterior boundary voxel and uses BFS to find all the connected voxels that are not already filled (essentially finding the air outside the object). We then take the inverse of these "air" voxels and the filled voxels with <code>inside_filled = ~flood_fill & ~filled</code> to fill in the empty voxels inside the object, and we assign these inside voxels a flesh colored pink color of <code>(255, 200, 200)</code>. 
      </p>
      <br>
      

      <h3>2. 3D Cellular Automata Neural Network</h3>
      <p class="small">
        Each voxel is built on 16 input channels: the first 4 are, in order, corresponding to RGBA values. The other 12 can be thought of as "hidden states" that convey information to their neighbours each update.  
        The model is built on three 3D-convolutions. The intuition behind the architecture is to first perceive from the sorroundings, and pool information from the 3x3x3 grid of neighbouring voxels. Next up, after a LayerNorm (for regularization purposes), we process the pooled information with layers with kernal size 1, eventually shrinking dimensionality to our desired output.
        </p>
			<p class="small">
				Initially, we trained our model to learn to grow — it started with a single black voxel (hidden state learned), and we optimized it to be able to construct the full mesh within 16-64 iterations (number of iterations is sampled uniformly). The model learns to do this relatively quickly, but does not learn to maintain the mesh — within a few more iterations, the mesh often degenerates into chaos. So, in the next stage of training, we start from the mesh created by the model, and optimize it to be able to maintain the mesh — this way, the model learns to grow our mesh, and maintain it. Now for the most interesting part: we made our mesh resilient to damage. This stage of training consists of randomly corrupting portions of the mesh, and training our model to be able to reconstruct these portions, resulting in a dynamic, living 3D object. 
			</p>
			<p class="small">
				We built a curriculum to be able to manage all these learning tasks, while still preventing catastrophic forgetting: every curriculum would add 64 iterations to the last one. So, 0->64, 64->128... upto 1024. 
			</p>
		<p class="small">
			Our model's loss function consists of 3 factors: undergrowth, overgrowth, and stability. Stability is on a linear schedule, since we want the model to just learn to grow initially. (Weights: undergrowth at 1, overgrowth at 10, and stability from 0->10).
		</p>      
        <div style="text-align:center;">
          <figure style="display:inline-block;">
            <img src="images/mario_epochs_1000.gif" alt="Animated GIF" width="400">
            <figcaption>Preliminary Model Results</figcaption>
          </figure>
        </div>

      <h3>3. Interactive Voxel Rendering and Model Evaluation</h3>
      <p class="small">
        Once the model has stabilized, we can visualize our NCA with a custom interactive GUI built with <code>VisPy</code> and <code>PyQt</code>. <code>Vispy</code> is a high-performance <code>Python</code> library powered by OpenGL, ideal for rendering large 2D and 3D visualizations like voxel grids. Its compatibility with <code>PyTorch</code> and <code>PyQt</code> made it well-suited for integrating real-time model inference with an interactive GUI. To get the interactive renderer working, we had to implement several key components, including voxel rendering, camera control, and mouse-based interaction: 
      </p>
      <ol>
        <li><i>Rendering the voxel grid</i></li>
        <ul>
          <p class = "small">
            <li>We first set up our <code>VisPy</code> canvas with a turntable camera to allow for interactive zooming and rotation. Next, we loaded our <code>PyTorch</code> model from the trained checkpoint set up a simulation function that would run model inference at every time step and outputs a 4D <code>NumPy</code> array of shape <code>(X,Y,Z,4)</code>.
            <br><br>
            <code>X,Y,Z</code> represents the spatial dimensions of the voxel grid and <code>4</code> represents the colors channels <code>R,G,B,A</code> where <code>A</code> is the alpha channel to determine the opacity of the color. To determine which voxels were "alive" at each time step, we used a simple thresholding method to determine whether the alpha channel of each voxel was above a certain "alive threshold" value, and only grabbed the <code>R,G,B</code> colors of the alive voxels. </li>
          <br>
          
          <li>Once we had the coordinates and colors of the alive voxels, we originally tried rendering the grid as a point cloud using the built-in <code>Markers</code> library. Although it was simple and easy to implement, the point cloud wasn't up to par with the desired rendering quality as pictured below:</li>

          <br><br>
            <figure style="display: block; margin: auto; text-align: center;">
              <img src="images/pointcloud.gif" alt="Markers" style="display: block; margin: auto;" width="250">
              <figcaption>Initial Point Cloud Attempt</figcaption>
            </figure>
          <br><br>
          <li>Next, we tried rendering each indiviudal voxel as a <code>Box</code> object, but rendering \(32^3\) individual cubes created a lot of lag. To improve the rendering speed, we decided to batch all the voxels together into a "mesh" and use the <code>MeshVisual</code> library and update the mesh data at each time step. This allowed us to create a very fast and visually appealing rendering (local on the CPU) while still maintaining the cube look, as shown below: </li>
          </p>
            <figure style="display: block; margin: auto; text-align: center;">
              <img src="images/mesh.gif" alt="MeshVisual" style="display: block; margin: auto;" width="250">
              <figcaption>Rendering with a mesh</figcaption>
            </figure>

          <p class = "small">
            <li>We removed shading on the mesh voxel object in order to ensure all faces of the object were uniformally lit. This caused the coloring of some voxels to look overly saturated, so we dialed down the saturation by converting the RGB colors to HSV, reducing the saturation, and converting back to RGB.</li>
          </p>
        </ul>

        <li><i>Transforming cursor clicks to 3D space</i></li>
          <ul>
            <p class = "small">
              <li>After handling basic rendering of the NCA model, the next step was to add user interaction with the grid through clicking. The goal was to transform a 2D cursor click into a ray in 3D space, but this was complicated by the interactive camera. However, we were able to use the built-in <code>view.scene.transform</code> object, which represents the current mapping between scene and screen coordinates in <code>VisPy</code>, and leveraged the inverse transformation <code>view.scene.transform.imap</code> matrix to transform points from the screen to world. 
              <br><br>
              To apply this transformation, we first get the <code>(x,y)</code> mouse position in screen coordinates and create two homogeneous coordinates to represent a near point and a far point on the viewing z-axis where 0 = near and 1 = far. 
              \[p_{near} = (x,y,0,1), p_{far} = (x,y,1,1)\]
              We apply the <code>imap</code> inverse transformation to both points to get the 3D coordinates of the near and far points in world coordinates, and set the ray origin to be the near point and the direction to be the normalized difference between the far and near points.
              \[ray_{origin} = imap(p_{near}[:3]), ray_{direction} = \frac{imap(p_{far}[:3]) - imap(p_{near}[:3])}{|imap(p_{far}[:3]) - imap(p_{near}[:3])|}\]
              We tested our transformation by drawing the resulting ray in 3D space, and it correctly aligned with our mouse clicks.
              <figcaption style="display: block; margin: auto; text-align: center;">
                <img src="images/draw_ray.jpg" alt="Ray Casting" style="display: block; margin: auto;" width="250">
                <figcaption>Testing Ray Casting from cursor</figcaption>
            </p>
          </ul>

        <li><i>Handle ray intersections with the voxel grid and damaging the voxel grid</i></li>
          <ul>
            <p class = "small">
              <li>To handle ray intersections with the voxel grid, we used a simple ray-casting algorithm to check for intersections between the ray and the voxel grid. Given the direction and origin of the mouse click ray, we iterate through all alive voxels, compute the voxel center, project that voxel center onto the ray, and measure the shortest distance from the voxel center to the ray using the ray equation described in lecture. 
              \[p(t) = ray_{origin} + t \cdot ray_{direction}\] 

              If the distance is within a threshold radius of the voxel center, we consider that voxel to be "hit" by the ray, and we take the closest of the hit voxels to be destroyed. Destruction is handled by zeroing out the voxel grid and living mask for the 6x6x6 cube of voxels around the hit voxel, which erases all features within that region. This allows us to visualize how the NCA model regenerates the voxel grid in the destroyed region. Destruction mode is enabled by holding down the "d" key, which also disables the camera controls. We also implemented support for click and drag to destroy multiple voxels at once by keeping track of mouse movement and checking for intersections with the ray at each time step.
              </li>

              <figcaption style="display: block; margin: auto; text-align: center;">
                <img src="images/duck_destroy.gif" alt="Voxel Damage" style="display: block; margin: auto;" width="250">
                <figcaption>Example of Voxel Damage and Regeneration</figcaption>
            </p>
          </ul>

        <li><i>User experience</i></li>
          <ul>
            <p class = "small">
              <li>We also added various buttons with <code>PyQt</code> to improve the user experience of the interactive renderer. We added a button to toggle between different models to visualize different objects at different resolutions, and also added a play, pause, and reset button to control the simulation. The play button starts the simulation and runs the model inference at each time step, while the pause button stops the simulation and allows the user to interact with the voxel grid. The reset button resets the voxel grid to its original state and stops the simulation.</li>
            </p>
          </ul>
      </ol>
      <br>
    <div style="text-align: center;">
        
      <h2 style="text-align: center;">Results</h2>

      <table style="margin: 0 auto; text-align: center; border-collapse: collapse;">
          <tr>
            <td>
              <figure>
                <img src="images/best_mario.gif" alt="Best Mario" width="400">
                <figcaption>Best Mario (32x32x32)</figcaption>
              </figure>
            </td>
            <td>
              <figure>
                <img src="images/duck_results.gif" alt="Duck Results" width="400">
                <figcaption>Duck (32x32x32) </figcaption>
              </figure>
            </td>
          </tr>
          <tr>
            <td>
              <figure>
                <img src="images/mariokart.gif" alt="Mario Kart" width="400">
                <figcaption>Mario Kart (32x32x32)</figcaption>
              </figure>
            </td>
            <td>
              <figure>
                <img src="images/baldmario.gif" alt="Bald Mario" width="400">
                <figcaption>Bald Mario - flood fill blooper (32x32x32)</figcaption>
              </figure>
            </td>
          </tr>
          <tr>
            <td>
              <figure>
                <img src="images/big_mario.gif" alt="Mario 64" width="400">
                <figcaption>Big Mario (64x64x64)</figcaption>
              </figure>
            </td>

          </tr>
        </table>
      </div>

		<h3>4. Training Infrastructure and Quantization</h3>
		<ol>
			<li><b>GPU Infrastructure</b></li>
			<br>
			<ul>Our models are divided into two types: the 32x32x32 environment ones and the 64x64x64 environment ones. Most of
				our initial experiments were done on 32x32x32 environment models and for these we used NVIDIA RTX 2080ti GPUs with 12GB of VRAM. Training on these
				took up approximately 10GB of VRAM and we were able to extend it to multi-GPU training through Distributed Data Parallel(DDP). This allowed us to train 
				faster on multiple GPUs, or train with larger batch sizes.
				<br><br>
				After reaching stable training on our 32x32x32 environments, we decided to scale up to 64x64x64 using NVIDIA H100 GPUs. We realized even with 80GB of VRAM, this large environment was reaching approximately
				70GB of VRAM. Therefore, we decided to explore options for efficiently scaling our training runs.
			 </ul>
			 <br>
			 <li><b>Half Precision Training</b></li>
			 <br>
			 <ul>Until now, we were running all training runs on full precision(FP32). Inspired by LLMs being frequently trained on half-precision(FP16) formats,
				we decided to try this out with our models, especially when running our larger models. We achieved this through PyTorch's Automatic Mixed Precision framework, with special layers like LayerNorm not being quantized down
				since it was specially sensitive to precision.
				<br><br>
				Although we got half precision training going, with an increased throughput of 1.4x, the model training was suffering serverly from overblowing NaNs. Though we tried mitigations such as reducing learning rates by two order of magnitudes, 
				running hyperparameter sweeps, or incorporating gradient clipping, avoiding NaNs blowing up was difficult to achieve. Even though we had some good runs where NaNs were avoided, the training was highly unstable when the model entered the curicculum learning phase. Here's an example loss curve below.
				<br><br>
				<div style="display: flex; justify-content: center; gap: 20px; text-align: center;">
					<figure style="width: 45%;">
						<img src="images/bad_fp16_trainlog.png" style="width: 100%;">
						<figcaption>FP16 Training Log</figcaption>
					</figure>
					<figure style="width: 45%;">
						<img src="images/64-fp16_2x.gif" style="width: 100%;">
						<figcaption>FP16 Output Visualization</figcaption>
					</figure>
				</div>				</ul>
				<br>
				<li><b>Post-Training Int8 Quantization</b></li>
				<br>
				<ul>Next on our list was deciding to go with full precision for training, but quantizating down the model after all the training was finished.
					Our goal was to have a live demo using our MacBooks as target hardware. We thought that quantizing the model weights down to Int8 would be incredibly fast and memory-efficient to run, given that 
					it could work with good generation accuracy. However, to our surprise, we realized the results were quite the oppsite: the model performance did not degrade after the quantization, but the inference speed decreased by a factor of approximately 3.61x.
					A summary of our inference time per model forward pass in included in the table below. All numbers are an average of 100 forward passes, and measured in MacBook Pro with the M1 Pro Chip.
					<br><br>
					(ENTER TABLE)
					baseline: 0.1840 sec
					int8 quantization: 0.6515 sec
					FP16 quantization: 
					<br><br>
					The direct reason behind why this happened is due to how Apple's M Series chips run FP32 and FP16 data types on their neural engine and GPUs, thus accelerating computations in such data formats, but fail to achieve similiar speedups for integer datatypes, despite
					even using Int8 datatypes. However, one flip-side is that we did achieve approximately 55% reduction in memory usage. Although, theoretically Int8 quantization achieves 75% reduction in memory, caveats such as leaving certain layers like LayerNorm in FP32 format lead to an actual
					of less than 75%.
				</ul>
				<br>
				<li><b>Post-Training FP16 Quantization</b></li>
				<br>
				<ul>The final approach we tried was running post-training quantization to FP16 instead of Int8. We expected this would accelerate computations by relying on Apple's Neural Engine and GPUs while maintaining manageable generation quality. However, when we ran the quantization to FP16, the model
					quality was degraded. We expect the performance degradation coming from the quantization also being applied to the LayerNorm layer. The model inference speed was indeed faster, reaching speeds of 4.38x compared to our baseline. Here are some example outputs comparing all approaches below.
				</ul>
				<br>
				<div style="display: flex; justify-content: center; gap: 20px; text-align: center;">
					<figure style="width: 30%;">
						<img src="images/mario64-fp32_2x.gif" style="width: 100%;">
						<figcaption>FP32</figcaption>
					</figure>
					<figure style="width: 30%;">
						<img src="images/mario-64-int8quantized_2x.gif" style="width: 100%;">
						<figcaption>INT8 Quantized</figcaption>
					</figure>
					<figure style="width: 30%;">
						<img src="images/64-fp16_2x.gif" style="width: 100%;">
						<figcaption>FP16</figcaption>
					</figure>
				</div>
		</ol>

    <b>insert model logs here </b>
    <br>

    <h2 style="text-align: center;">References</h2>
      <ul>
        <li><a href = "https://downloads.greyc.fr/Greyc3DColoredMeshDatabase/">GREYC Colored 3D Mesh Dataset</a></li>
        <li><a href="https://vispy.org/api/vispy.scene.html">VisPy Scene API</a></li>
        <li><a href="https://stackoverflow.com/questions/33942728/how-to-get-world-coordinates-from-screen-coordinates-in-vispy">How to Get World Coordinates from Screen Coordinates in VisPy</a></li>
        <li><a href="https://pythonbasics.org/pyqt-buttons/">PyQt Buttons</a></li>
        <li><a href="https://www.geeksforgeeks.org/flood-fill-algorithm/">Flood Fill Algorithm (GeeksforGeeks)</a></li>
			  <li><a href="https://distill.pub/2020/growing-ca/#experiment-3">2D Growing Neural Cellular Automata (distill.pub/2020/growing-ca/#experiment-3)</a></li>
			  <li><a href="https://github.com/Aadityaza/3d-Growing-neural-cellular-automata">3D Growing Neural Cellular Automata</a></li>
			  <li><a href="https://meshnca.github.io/">Mesh Neural Cellular Automata</a></li>
			  <li><a href="https://wandb.ai/johnowhitaker/nca/reports/Fun-With-Neural-Cellular-Automata--VmlldzoyMDQ5Mjg0">Fun with Neural Cellular Automata</a></li>
			  <li><a href="https://greydanus.github.io/2022/05/24/studying-growth/">Studying Growth with Neural Cellular Automata</a></li>
			  <li><a href="https://playgameoflife.com/">Conway's Game of Life</a></li>
			  <li><a href="https://arxiv.org/pdf/2103.08737">Growing 3D Artefacts and Functional Machines with Neural Cellular Automata</a></li>
		  </ul>
    <br>

    <h2>Contributions</h2>
    <enumerate>
      <li>Annabel Ng: Developed the 3D mesh to 3D voxel pipeline as well as the interactive voxel rendering</li>
      <li>George Rickus: Model design, model training, and model stabilization</li>
      <li>Henry Ko: 3D mesh to 3D voxel pipeline, model quantization, model training</li>
      <li>Samarth Jajoo: Model training, damage resilience</li>
    </enumerate>


		</div>
	</body>
</html>